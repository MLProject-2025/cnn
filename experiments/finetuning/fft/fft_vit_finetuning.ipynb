{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfjvgnO1hAYo",
        "outputId": "3afbd51d-71a2-4a9f-9d41-b3fc030401ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "===== START =====\n",
            "DEVICE: cuda\n",
            "Real: 34681, Fake: 33854\n",
            "\n",
            "[샘플링] 최대 10000개 중 10000개 사용\n",
            "→ 샘플링 완료: 10000개\n",
            "\n",
            "Train 7000  Val 2000  Test 1000\n",
            "\n",
            "=== Loading vit (pretrained, mode=full, use_fft=True) ===\n",
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 330M/330M [00:05<00:00, 66.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "→ 전체 파라미터 Fine-Tuning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 1/30: 100%|██████████| 110/110 [08:19<00:00,  4.54s/it]\n",
            "Val Epoch 1/30: 100%|██████████| 32/32 [00:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] Train Loss 0.7127 Acc 0.5186 | Val Loss 0.7014 Acc 0.5060\n",
            "→ Best model 갱신\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 2/30: 100%|██████████| 110/110 [04:01<00:00,  2.20s/it]\n",
            "Val Epoch 2/30: 100%|██████████| 32/32 [00:23<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 2] Train Loss 0.6873 Acc 0.5519 | Val Loss 0.6817 Acc 0.5795\n",
            "→ Best model 갱신\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 3/30: 100%|██████████| 110/110 [03:57<00:00,  2.16s/it]\n",
            "Val Epoch 3/30: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 3] Train Loss 0.6657 Acc 0.5931 | Val Loss 0.6566 Acc 0.5980\n",
            "→ Best model 갱신\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 4/30: 100%|██████████| 110/110 [03:59<00:00,  2.17s/it]\n",
            "Val Epoch 4/30: 100%|██████████| 32/32 [00:23<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 4] Train Loss 0.6698 Acc 0.5864 | Val Loss 0.6990 Acc 0.5165\n",
            "→ 개선 없음 (1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 5/30: 100%|██████████| 110/110 [03:58<00:00,  2.17s/it]\n",
            "Val Epoch 5/30: 100%|██████████| 32/32 [00:23<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 5] Train Loss 0.6735 Acc 0.5749 | Val Loss 0.6893 Acc 0.5120\n",
            "→ 개선 없음 (2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch 6/30: 100%|██████████| 110/110 [03:58<00:00,  2.17s/it]\n",
            "Val Epoch 6/30: 100%|██████████| 32/32 [00:24<00:00,  1.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 6] Train Loss 0.6554 Acc 0.6051 | Val Loss 0.7283 Acc 0.5965\n",
            "→ 개선 없음 (3/3)\n",
            "EARLY STOPPING!\n",
            "▶ 학습 곡선 이미지 저장 완료: /content/train_history_loss.png, /content/train_history_acc.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "TEST: 100%|██████████| 16/16 [00:12<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.6584\n",
            "Test Accuracy: 60.60%\n",
            "▶ Confusion Matrix 이미지 저장 완료: /content/confusion_matrix.png\n",
            "===== DONE =====\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torchvision.models import (\n",
        "    googlenet, GoogLeNet_Weights,\n",
        "    resnet50, ResNet50_Weights,\n",
        "    vgg16, VGG16_Weights,\n",
        "    vit_b_16, ViT_B_16_Weights\n",
        ")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/기학기\"\n",
        "REAL_PATH = BASE_DIR + \"/face_real\"\n",
        "FAKE_PATH = BASE_DIR + \"/face_fake\"\n",
        "MODEL_SAVE_PATH = \"/content/best_googlenet_fft.pth\"\n",
        "\n",
        "IMG_SIZE = 224\n",
        "EPOCHS = 30\n",
        "BATCH_SIZE = 64\n",
        "NUM_SAMPLES = 10000\n",
        "LEARNING_RATE = 1e-4\n",
        "PATIENCE = 3\n",
        "\n",
        "# 2. ResizeWithPad\n",
        "class ResizeWithPad:\n",
        "    \"\"\"긴 변 기준으로 리사이즈 후, 짧은 변은 패딩해서 정사각형으로 맞추는 Transform (PIL -> PIL)\"\"\"\n",
        "    def __init__(self, target_size):\n",
        "        self.target_size = target_size\n",
        "\n",
        "    def __call__(self, img):\n",
        "        w, h = img.size\n",
        "        scale = self.target_size / max(w, h)\n",
        "\n",
        "        new_w = int(w * scale)\n",
        "        new_h = int(h * scale)\n",
        "\n",
        "        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n",
        "        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n",
        "\n",
        "        pad_x = (self.target_size - new_w) // 2\n",
        "        pad_y = (self.target_size - new_h) // 2\n",
        "        canvas.paste(resized_img, (pad_x, pad_y))\n",
        "\n",
        "        return canvas\n",
        "\n",
        "\n",
        "# 3. FFT Magnitude Transform\n",
        "class FFTMag:\n",
        "\n",
        "    def __call__(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x_fft = torch.fft.fft2(x)\n",
        "        x_fft_shift = torch.fft.fftshift(x_fft)\n",
        "        mag = torch.abs(x_fft_shift)\n",
        "        mag = torch.log1p(mag)\n",
        "\n",
        "        mean = mag.mean()\n",
        "        std = mag.std()\n",
        "        mag = (mag - mean) / (std + 1e-8)\n",
        "\n",
        "        return mag\n",
        "\n",
        "# 4. Dataset\n",
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform=None):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.paths[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
        "        return img, label.unsqueeze(0)\n",
        "\n",
        "\n",
        "# 5. Fine-Tuning 모델\n",
        "def get_model_finetune(name, device, mode=\"full\"):\n",
        "    \"\"\"\n",
        "    name: 'googlenet', 'resnet50', 'vgg16', 'vit' 중 하나\n",
        "    mode: 'head' -> classifier만 학습, 'full' -> 전체 파라미터 학습\n",
        "    \"\"\"\n",
        "    print(f\"\\n=== Loading {name} (pretrained, mode={mode}, use_fft=True) ===\")\n",
        "\n",
        "    if name == \"googlenet\":\n",
        "        weights = GoogLeNet_Weights.DEFAULT\n",
        "        model = googlenet(weights=weights)\n",
        "        in_f = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_f, 1)\n",
        "        head_params = model.fc.parameters()\n",
        "\n",
        "    elif name == \"resnet50\":\n",
        "        weights = ResNet50_Weights.DEFAULT\n",
        "        model = resnet50(weights=weights)\n",
        "        in_f = model.fc.in_features\n",
        "        model.fc = nn.Linear(in_f, 1)\n",
        "        head_params = model.fc.parameters()\n",
        "\n",
        "    elif name == \"vgg16\":\n",
        "        weights = VGG16_Weights.DEFAULT\n",
        "        model = vgg16(weights=weights)\n",
        "        in_f = model.classifier[6].in_features\n",
        "        model.classifier[6] = nn.Linear(in_f, 1)\n",
        "        head_params = model.classifier[6].parameters()\n",
        "\n",
        "    elif name == \"vit\":\n",
        "        weights = ViT_B_16_Weights.DEFAULT\n",
        "        model = vit_b_16(weights=weights)\n",
        "        in_f = model.heads.head.in_features\n",
        "        model.heads.head = nn.Linear(in_f, 1)\n",
        "        head_params = model.heads.head.parameters()\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"지원하지 않는 모델 이름입니다.\")\n",
        "\n",
        "    # Freeze 설정\n",
        "    if mode == \"head\":\n",
        "        print(\"→ Classifier만 학습 (Feature extractor는 Freeze)\")\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = False\n",
        "        for p in head_params:\n",
        "            p.requires_grad = True\n",
        "    else:\n",
        "        print(\"→ 전체 파라미터 Fine-Tuning\")\n",
        "        for p in model.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "# 6. Train (히스토리 저장)\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n",
        "\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_model = None\n",
        "    no_improve = 0\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_acc\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total, correct = 0, 0\n",
        "        running_loss = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Train Epoch {epoch+1}/{epochs}\")\n",
        "        for X, y in pbar:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out = model(X)\n",
        "\n",
        "            if hasattr(out, \"logits\"):\n",
        "                out = out.logits\n",
        "            elif isinstance(out, (tuple, list)):\n",
        "                out = out[0]\n",
        "\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            pred = (torch.sigmoid(out) > 0.5).float()\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        train_acc = correct / total\n",
        "\n",
        "        # ---------------------- Validation --------------------------\n",
        "        model.eval()\n",
        "        total, correct = 0, 0\n",
        "        running_val = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in tqdm(val_loader, desc=f\"Val Epoch {epoch+1}/{epochs}\"):\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                out = model(X)\n",
        "\n",
        "                if hasattr(out, \"logits\"):\n",
        "                    out = out.logits\n",
        "                elif isinstance(out, (tuple, list)):\n",
        "                    out = out[0]\n",
        "\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                running_val += loss.item() * X.size(0)\n",
        "                pred = (torch.sigmoid(out) > 0.5).float()\n",
        "                correct += (pred == y).sum().item()\n",
        "                total += y.size(0)\n",
        "\n",
        "        val_loss = running_val / len(val_loader.dataset)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Loss {train_loss:.4f} Acc {train_acc:.4f} | \"\n",
        "              f\"Val Loss {val_loss:.4f} Acc {val_acc:.4f}\")\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_model = copy.deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "            print(\"→ Best model 갱신\")\n",
        "            torch.save(best_model, MODEL_SAVE_PATH)\n",
        "        else:\n",
        "            no_improve += 1\n",
        "            print(f\"→ 개선 없음 ({no_improve}/{patience})\")\n",
        "            if no_improve >= patience:\n",
        "                print(\"EARLY STOPPING!\")\n",
        "                break\n",
        "\n",
        "    if best_model is not None:\n",
        "        model.load_state_dict(best_model)\n",
        "    return model, history\n",
        "\n",
        "\n",
        "# 7. 학습 곡선 시각화\n",
        "def plot_history(history, save_prefix=\"/content/train_history\"):\n",
        "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
        "\n",
        "    # Loss\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs, history[\"val_loss\"], label=\"Val Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Train / Val Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_prefix + \"_loss.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(epochs, history[\"train_acc\"], label=\"Train Acc\")\n",
        "    plt.plot(epochs, history[\"val_acc\"], label=\"Val Acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Train / Val Accuracy\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_prefix + \"_acc.png\")\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"▶ 학습 곡선 이미지 저장 완료: {save_prefix}_loss.png, {save_prefix}_acc.png\")\n",
        "\n",
        "\n",
        "# 8. Evaluate + Confusion Matrix\n",
        "def evaluate_model(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    running_loss = 0\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in tqdm(loader, desc=\"TEST\"):\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            out = model(X)\n",
        "\n",
        "            if hasattr(out, \"logits\"):\n",
        "                out = out.logits\n",
        "            elif isinstance(out, (tuple, list)):\n",
        "                out = out[0]\n",
        "\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            running_loss += loss.item() * X.size(0)\n",
        "            probs = torch.sigmoid(out)\n",
        "            pred = (probs > 0.5).float()\n",
        "            correct += (pred == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "            all_preds.append(pred.cpu().numpy())\n",
        "\n",
        "    test_loss = running_loss / len(loader.dataset)\n",
        "    test_acc = correct / total\n",
        "\n",
        "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
        "\n",
        "    y_true = np.concatenate(all_labels, axis=0)\n",
        "    y_pred = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure()\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=\"Blues\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(2)\n",
        "    plt.xticks(tick_marks, [\"Real(0)\", \"Fake(1)\"])\n",
        "    plt.yticks(tick_marks, [\"Real(0)\", \"Fake(1)\"])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                     ha=\"center\", va=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    cm_path = \"/content/confusion_matrix.png\"\n",
        "    plt.savefig(cm_path)\n",
        "    plt.close()\n",
        "    print(f\"▶ Confusion Matrix 이미지 저장 완료: {cm_path}\")\n",
        "\n",
        "\n",
        "# 9. MAIN\n",
        "def main():\n",
        "    print(\"===== START =====\")\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"DEVICE:\", device)\n",
        "\n",
        "    real_paths = glob.glob(os.path.join(REAL_PATH, \"*\"))\n",
        "    fake_paths = glob.glob(os.path.join(FAKE_PATH, \"*\"))\n",
        "\n",
        "    print(f\"Real: {len(real_paths)}, Fake: {len(fake_paths)}\")\n",
        "\n",
        "    if len(real_paths) == 0 or len(fake_paths) == 0:\n",
        "        print(\"❌ Real 또는 Fake 폴더에 이미지가 없습니다. 경로 다시 확인하세요.\")\n",
        "        return\n",
        "\n",
        "    all_paths = real_paths + fake_paths\n",
        "    all_labels = [0]*len(real_paths) + [1]*len(fake_paths)\n",
        "\n",
        "    use_num = min(NUM_SAMPLES, len(all_paths))\n",
        "    print(f\"\\n[샘플링] 최대 {NUM_SAMPLES}개 중 {use_num}개 사용\")\n",
        "\n",
        "    if use_num == len(all_paths):\n",
        "        sample_paths = all_paths\n",
        "        sample_labels = all_labels\n",
        "        print(\"→ 전체 데이터 사용\")\n",
        "    else:\n",
        "        sample_paths, _, sample_labels, _ = train_test_split(\n",
        "            all_paths, all_labels,\n",
        "            train_size=use_num,\n",
        "            stratify=all_labels,\n",
        "            random_state=42\n",
        "        )\n",
        "        print(f\"→ 샘플링 완료: {len(sample_paths)}개\")\n",
        "\n",
        "    train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "        sample_paths, sample_labels, test_size=0.3, stratify=sample_labels, random_state=42\n",
        "    )\n",
        "    val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "        temp_paths, temp_labels, test_size=1/3, stratify=temp_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTrain {len(train_paths)}  Val {len(val_paths)}  Test {len(test_paths)}\")\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        ResizeWithPad(IMG_SIZE),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        FFTMag()\n",
        "    ])\n",
        "\n",
        "    test_tf = transforms.Compose([\n",
        "        ResizeWithPad(IMG_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        FFTMag()\n",
        "    ])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        DeepfakeDataset(train_paths, train_labels, train_tf),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        DeepfakeDataset(val_paths, val_labels, test_tf),\n",
        "        batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        DeepfakeDataset(test_paths, test_labels, test_tf),\n",
        "        batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=2, pin_memory=True\n",
        "    )\n",
        "\n",
        "    # --- 모델/옵티마 정의 ---\n",
        "    model = get_model_finetune(\"vit\", device, mode=\"full\")\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(\n",
        "        filter(lambda p: p.requires_grad, model.parameters()),\n",
        "        lr=LEARNING_RATE\n",
        "    )\n",
        "\n",
        "    # --- 학습 ---\n",
        "    model, history = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n",
        "\n",
        "    # --- 학습 곡선 시각화 ---\n",
        "    plot_history(history, save_prefix=\"/content/train_history\")\n",
        "\n",
        "    # --- 평가 + Confusion Matrix ---\n",
        "    evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "    print(\"===== DONE =====\")\n",
        "\n",
        "\n",
        "# 실행\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "680d70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8830f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ì‹¤í—˜ ëª¨ë“œ ì„ íƒ] \n",
    "# EXP_NAME = 'BASELINE'  # 1. ê¸°ë³¸ RGB (ê³¼ì í•© í™•ì¸ìš©)\n",
    "EXP_NAME = 'ROBUST'    # 2. RGB + Mixup + Strong Aug (ê°•ê±´ì„±)\n",
    "# EXP_NAME = 'FREQ'      # 3. Frequency + Mixup (ì•„í‹°íŒ©íŠ¸)\n",
    "\n",
    "# [ë°ì´í„° ê²½ë¡œ ì„¤ì •]\n",
    "# í•™ìŠµìš©: 6ë§Œì¥ ë°ì´í„° (í´ë” ì•ˆì— Real, Fake í˜¹ì€ 0, 1 í´ë” ì¡´ì¬ ê°€ì •)\n",
    "TRAIN_DATA_ROOT = '../datasets/deepfake/Train' \n",
    "\n",
    "# í‰ê°€ìš©: Wiki/Inpainting ë“± Cropped ë°ì´í„° (í•™ìŠµì— ì•ˆ ì“´ ê²ƒ!)\n",
    "TEST_DATA_ROOT = '../datasets/deepfake_60k_cropped_224px'\n",
    "\n",
    "# TRAIN_DATA_ROOT, TEST_DATA_ROOT = TEST_DATA_ROOT, TRAIN_DATA_ROOT \n",
    "\n",
    "TRAIN_N_SAMPLES = 60000\n",
    "TEST_N_SAMPLES = 10000\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64 # ë©”ëª¨ë¦¬ í„°ì§€ë©´ 32ë¡œ\n",
    "EPOCHS = 15\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5c674",
   "metadata": {},
   "source": [
    "## ì „ì²˜ë¦¬ ë° ì¦ê°• ë¡œì§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0730748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ì£¼íŒŒìˆ˜ ë„ë©”ì¸ ë³€í™˜ (uint8 -> uint8)\"\"\"\n",
    "# def get_freq_feature(img_np):\n",
    "#     blur = cv2.GaussianBlur(img_np, (21, 21), 0)\n",
    "#     high_pass = cv2.addWeighted(img_np, 4, blur, -4, 128)\n",
    "#     fft_img = np.zeros_like(img_np, dtype=np.float32)\n",
    "#     for c in range(3):\n",
    "#         f = np.fft.fft2(high_pass[:, :, c])\n",
    "#         fshift = np.fft.fftshift(f)\n",
    "#         magnitude = 20 * np.log(np.abs(fshift) + 1e-8)\n",
    "#         fft_img[:, :, c] = magnitude\n",
    "#     return cv2.normalize(fft_img, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "\n",
    "# [ìˆ˜ì • í›„: ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ë§µ(Residual)ë§Œ ë°˜í™˜]\n",
    "def get_freq_feature(img_np):\n",
    "    \"\"\"\n",
    "    FFT ë³€í™˜ì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³ , ê³µê°„ ë„ë©”ì¸ì˜ ê³ ì£¼íŒŒ ì”ì°¨(Residual)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    CNNì´ í›¨ì”¬ í•™ìŠµí•˜ê¸° ì‰¬ìš°ë©´ì„œë„, Content(ì–¼êµ´)ê°€ ì•„ë‹Œ Artifact(ë…¸ì´ì¦ˆ)ì— ì§‘ì¤‘í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    # 1. Grayscale ë³€í™˜ (ìƒ‰ìƒ ì •ë³´ ì œê±°, í…ìŠ¤ì²˜ ì§‘ì¤‘)\n",
    "    gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2. ê³ ì£¼íŒŒ ì¶”ì¶œ (High Pass Filter)\n",
    "    # ì›ë³¸ - ë¸”ëŸ¬ = ì—£ì§€ì™€ ë…¸ì´ì¦ˆë§Œ ë‚¨ìŒ\n",
    "    blur = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    high_pass = cv2.addWeighted(gray, 1.5, blur, -0.5, 0) # ìƒ¤í”ˆ í•„í„° ëŠë‚Œìœ¼ë¡œ ê°•í™”\n",
    "    \n",
    "    # í˜¹ì€ ë¼í”Œë¼ì‹œì•ˆ í•„í„° (ë” ê°•ë ¥í•œ ì—£ì§€ ê²€ì¶œ)\n",
    "    # high_pass = cv2.Laplacian(gray, cv2.CV_16S, ksize=3)\n",
    "    # high_pass = cv2.convertScaleAbs(high_pass)\n",
    "    \n",
    "    # 3. ì°¨ì›ì„ 3ì±„ë„ë¡œ ë§ì¶°ì¤Œ (ResNet ì…ë ¥ í˜¸í™˜ì„± ìœ„í•´)\n",
    "    high_pass_3c = cv2.cvtColor(high_pass, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # 4. íˆìŠ¤í† ê·¸ë¨ í‰í™œí™” (ë…¸ì´ì¦ˆ ëŒ€ë¹„ ê·¹ëŒ€í™” - ì˜µì…˜)\n",
    "    # high_pass_3c[:,:,0] = cv2.equalizeHist(high_pass_3c[:,:,0])\n",
    "    # high_pass_3c[:,:,1] = cv2.equalizeHist(high_pass_3c[:,:,1])\n",
    "    # high_pass_3c[:,:,2] = cv2.equalizeHist(high_pass_3c[:,:,2])\n",
    "\n",
    "    return high_pass_3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cd388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ê°•ë ¥í•œ í™”ì§ˆ ì €í•˜ ì¦ê°• (í•™ìŠµ ì‹œì—ë§Œ)\"\"\"\n",
    "def strong_aug(img_np):\n",
    "    if random.random() < 0.5: # JPEG Compression\n",
    "        q = random.randint(30, 90)\n",
    "        _, enc = cv2.imencode('.jpg', img_np, [int(cv2.IMWRITE_JPEG_QUALITY), q])\n",
    "        img_np = cv2.imdecode(enc, 1)\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    if random.random() < 0.3: # Blur\n",
    "        img_np = cv2.GaussianBlur(img_np, (5, 5), 0)\n",
    "        \n",
    "    if random.random() < 0.3: # Noise\n",
    "        noise = np.random.normal(0, 15, img_np.shape).astype(np.uint8)\n",
    "        img_np = np.clip(img_np.astype(np.int16) + noise, 0, 255).astype(np.uint8)\n",
    "        \n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dba32be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"mixup ê¸°ë²• ì ìš© (labelì´ [0, 1] ì´ë©´ [0.1, 0.9] ì´ëŸ°ì‹ìœ¼ë¡œ ë°”ê¿ˆ)\"\"\"\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0: lam = np.random.beta(alpha, alpha)\n",
    "    else: lam = 1\n",
    "    index = torch.randperm(x.size(0)).to(DEVICE)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155582dc",
   "metadata": {},
   "source": [
    "## data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea9c299d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_class_dirs(root_dir):\n",
    "\n",
    "    print(f\"Scanning: {os.path.abspath(root_dir)}\") # ì ˆëŒ€ ê²½ë¡œ í™•ì¸ìš© ì¶œë ¥\n",
    "    \n",
    "    if not os.path.exists(root_dir):\n",
    "        print(f\"[Error] ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {root_dir}\")\n",
    "        print(\"  -> í˜„ì¬ ì‘ì—… ê²½ë¡œ(os.getcwd()):\", os.getcwd())\n",
    "        return None, None\n",
    "\n",
    "    real_dir, fake_dir = None, None\n",
    "    \n",
    "    # 1. ë£¨íŠ¸ ë°”ë¡œ ì•„ë˜ íƒìƒ‰\n",
    "    subdirs = [os.path.join(root_dir, d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))]\n",
    "    \n",
    "    # 2. ë§Œì•½ ë°”ë¡œ ì•„ë˜ì— ì—†ë‹¤ë©´ 1 depth ë” ë“¤ì–´ê°€ì„œ íƒìƒ‰ (ì˜ˆ: Train/Train/Real...)\n",
    "    if len(subdirs) == 1:\n",
    "        nested_root = subdirs[0]\n",
    "        print(f\"  -> Checking nested dir: {nested_root}\")\n",
    "        subdirs = [os.path.join(nested_root, d) for d in os.listdir(nested_root) if os.path.isdir(os.path.join(nested_root, d))]\n",
    "\n",
    "    for d in subdirs:\n",
    "        name = os.path.basename(d).lower()\n",
    "        if 'real' in name: real_dir = d\n",
    "        elif 'fake' in name: fake_dir = d\n",
    "            \n",
    "    return real_dir, fake_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "245348dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ë°ì´í„° ì „ì²´ë¥¼ RAMì— ë¡œë“œí•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
    "def load_data_to_ram(root_dir, target_size=224, n_samples=None):\n",
    "    print(f\"Loading data from {root_dir} into RAM...\")\n",
    "    \n",
    "    real_dir, fake_dir = find_class_dirs(root_dir)\n",
    "    if not real_dir or not fake_dir:\n",
    "        print(f\"[Error] '{root_dir}'ì—ì„œ Real/Fake í´ë”ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    print(f\" - Real: {os.path.basename(real_dir)}\")\n",
    "    print(f\" - Fake: {os.path.basename(fake_dir)}\")\n",
    "\n",
    "    # 1. ëª¨ë“  ê²½ë¡œ ìˆ˜ì§‘\n",
    "    all_files = []\n",
    "    for label, folder in enumerate([real_dir, fake_dir]): # 0: Real, 1: Fake\n",
    "        files = glob.glob(os.path.join(folder, \"*.*\"))\n",
    "        # ì´ë¯¸ì§€ í™•ì¥ì í•„í„°ë§\n",
    "        files = [f for f in files if f.lower().endswith(('.jpg', '.png', '.jpeg', '.webp'))]\n",
    "        for f in files:\n",
    "            all_files.append((f, label))\n",
    "            \n",
    "    total_files = len(all_files)\n",
    "    print(f\" - Total Images Found: {total_files}\")\n",
    "    \n",
    "    # 2. ìƒ˜í”Œë§ (n_samples ì„¤ì • ì‹œ)\n",
    "    if n_samples is not None and n_samples < total_files:\n",
    "        print(f\" - Sampling {n_samples} images...\")\n",
    "        random.shuffle(all_files)\n",
    "        all_files = all_files[:n_samples]\n",
    "    else:\n",
    "        # ì…”í”Œì€ í•™ìŠµ ë°ì´í„° ë¶„í¬ë¥¼ ì„ì–´ì£¼ë¯€ë¡œ ê¸°ë³¸ì ìœ¼ë¡œ í•´ë‘ëŠ” ê²Œ ì¢‹ìŒ\n",
    "        random.shuffle(all_files)\n",
    "\n",
    "    # 3. ë¡œë”© ì‹œì‘\n",
    "    crop_transform = transforms.CenterCrop(target_size)\n",
    "    resize_transform = transforms.Resize((target_size, target_size))\n",
    "    to_uint8_tensor = transforms.PILToTensor() \n",
    "\n",
    "    error_count = 0 # ì—ëŸ¬ ì¹´ìš´íŠ¸ ì¶”ê°€\n",
    "    \n",
    "    tensor_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    for fpath, label in tqdm(all_files, desc=\"Loading to RAM\", leave=False):\n",
    "        try:\n",
    "            img = Image.open(fpath).convert('RGB')\n",
    "            w, h = img.size\n",
    "            \n",
    "            # 224ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©´ CenterCrop (í™”ì§ˆ ë³´ì¡´)\n",
    "            if w >= target_size and h >= target_size:\n",
    "                img = crop_transform(img)\n",
    "            else:\n",
    "                # ì‘ìœ¼ë©´ Resize\n",
    "                img = resize_transform(img)\n",
    "            \n",
    "            # uint8 Tensor ë³€í™˜\n",
    "            t_img = to_uint8_tensor(img)\n",
    "            \n",
    "            tensor_list.append(t_img)\n",
    "            label_list.append(label)\n",
    "        except Exception as e:\n",
    "             # [ë””ë²„ê¹…] ì²˜ìŒ 10ê°œ ì—ëŸ¬ë§Œ ì¶œë ¥\n",
    "            if error_count < 10:\n",
    "                print(f\"[Error] Failed to load {fpath}: {e}\")\n",
    "                error_count += 1\n",
    "            pass \n",
    "\n",
    "\n",
    "    if not tensor_list:\n",
    "        print(\"[Error] ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None, None\n",
    "\n",
    "    # Stack\n",
    "    data_tensor = torch.stack(tensor_list)\n",
    "    label_tensor = torch.tensor(label_list, dtype=torch.long)\n",
    "    \n",
    "    mem_size = data_tensor.element_size() * data_tensor.nelement() / (1024**3)\n",
    "    print(f\"Load Complete! {len(data_tensor)} images. RAM: {mem_size:.2f} GB\")\n",
    "    \n",
    "    return data_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5e637c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"dataset ì •ì˜\"\"\"\n",
    "class RAMDataset(Dataset):\n",
    "    def __init__(self, data_tensor, label_tensor, exp_name, is_train=True):\n",
    "        self.data = data_tensor   \n",
    "        self.labels = label_tensor\n",
    "        self.exp_name = exp_name\n",
    "        self.is_train = is_train\n",
    "        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        self.to_float = transforms.ToTensor() \n",
    "\n",
    "    def __len__(self): return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_tensor = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tensor -> Numpy (H, W, 3) for OpenCV\n",
    "        img_np = img_tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # Augmentation (Train)\n",
    "        if self.is_train and self.exp_name in ['ROBUST', 'FREQ']:\n",
    "            img_np = strong_aug(img_np)\n",
    "        \n",
    "        # Freq Conversion\n",
    "        if self.exp_name == 'FREQ':\n",
    "            img_np = get_freq_feature(img_np)\n",
    "            \n",
    "        # Numpy -> Float Tensor & Normalize\n",
    "        img_out = self.to_float(img_np)\n",
    "        img_out = self.normalize(img_out)\n",
    "        \n",
    "        return img_out, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7cdcc",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a856a15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Experiment: [ROBUST] (Target Size: 224)\n",
      ">>> Loading Train Set...\n",
      "Loading data from ../datasets/deepfake/Train into RAM...\n",
      "Scanning: /home/dongwook/workspace/datasets/deepfake/Train\n",
      " - Real: Real\n",
      " - Fake: Fake\n",
      " - Total Images Found: 140002\n",
      " - Sampling 60000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete! 60000 images. RAM: 8.41 GB\n",
      ">>> Loading Test Set...\n",
      "Loading data from ../datasets/deepfake_60k_cropped_224px into RAM...\n",
      "Scanning: /home/dongwook/workspace/datasets/deepfake_60k_cropped_224px\n",
      " - Real: Real\n",
      " - Fake: Fake\n",
      " - Total Images Found: 59949\n",
      " - Sampling 10000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete! 10000 images. RAM: 1.40 GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸš€ Experiment: [{EXP_NAME}] (Target Size: {IMG_SIZE})\")\n",
    "\n",
    "# 1. Load Data\n",
    "print(\">>> Loading Train Set...\")\n",
    "train_x, train_y = load_data_to_ram(TRAIN_DATA_ROOT, IMG_SIZE, TRAIN_N_SAMPLES)\n",
    "print(\">>> Loading Test Set...\")\n",
    "test_x, test_y = load_data_to_ram(TEST_DATA_ROOT, IMG_SIZE, TEST_N_SAMPLES)\n",
    "\n",
    "if train_x is None or test_x is None: \n",
    "    raise Exception(\"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "\n",
    "# 2. Loader (num_workers=0 for RAM speed)\n",
    "train_ds = RAMDataset(train_x, train_y, EXP_NAME, is_train=True)\n",
    "test_ds = RAMDataset(test_x, test_y, EXP_NAME, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c343c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model\n",
    "if EXP_NAME == 'FREQ':\n",
    "    model = models.resnet18(pretrained=False)\n",
    "else:\n",
    "    model = models.resnet18(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7030e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "        \n",
    "        if EXP_NAME != 'BASELINE':\n",
    "            inputs, ta, tb, lam = mixup_data(inputs, labels, alpha=0.4)\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, ta, tb, lam)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    preds, acts = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "            acts.extend(labels.numpy())\n",
    "            \n",
    "    acc = accuracy_score(acts, preds)\n",
    "    print(f\"Epoch {epoch+1} | Test Acc: {acc:.4f}\")\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), f\"model_ram_{EXP_NAME}.pth\")\n",
    "        print(\"--> Best Model Saved!\")\n",
    "\n",
    "print(f\"\\nâœ… Finished. Best Acc: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7846c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, acc):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Pred: Real', 'Pred: Fake'], \n",
    "                yticklabels=['Act: Real', 'Act: Fake'])\n",
    "    plt.title(f'Fusion Result (Acc: {acc:.4f})')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44759165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fusion():\n",
    "    SWAP_DATASETS = False\n",
    "    print(\"\\nğŸš€ Running Model Fusion (Ensemble)...\")\n",
    "    \n",
    "    # 1. Load Test Data (ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ë¡œë“œ)\n",
    "    # SWAP ì—¬ë¶€ì— ë”°ë¼ í…ŒìŠ¤íŠ¸ ê²½ë¡œ ìë™ ì„¤ì •\n",
    "    if 'SWAP_DATASETS' in globals() and SWAP_DATASETS:\n",
    "        target_path = TRAIN_DATA_ROOT # Kaggleì´ Testê°€ ë¨\n",
    "        target_n = TRAIN_N_SAMPLES\n",
    "        tag = \"SWAP\"\n",
    "        print(f\"   -> Mode: SWAP (Testing on Kaggle Data: {target_path})\")\n",
    "    else:\n",
    "        target_path = TEST_DATA_ROOT # Wikiê°€ Testê°€ ë¨\n",
    "        target_n = TEST_N_SAMPLES\n",
    "        tag = \"NORMAL\"\n",
    "        print(f\"   -> Mode: NORMAL (Testing on Wiki Data: {target_path})\")\n",
    "\n",
    "    # RAM ë¡œë”©\n",
    "    test_x, test_y = load_data_to_ram(target_path, IMG_SIZE, target_n)\n",
    "    if test_x is None: \n",
    "        print(\"[Error] í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    # 2. Create Loaders (ë™ì¼ ë°ì´í„°, ë‹¤ë¥¸ ì „ì²˜ë¦¬)\n",
    "    # ROBUSTìš© ë¡œë”\n",
    "    ds_robust = RAMDataset(test_x, test_y, EXP_NAME='ROBUST', is_train=False)\n",
    "    loader_robust = DataLoader(ds_robust, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # FREQìš© ë¡œë”\n",
    "    ds_freq = RAMDataset(test_x, test_y, EXP_NAME='FREQ', is_train=False)\n",
    "    loader_freq = DataLoader(ds_freq, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 3. Load Models\n",
    "    model_robust = models.resnet18(pretrained=False) # êµ¬ì¡°ë§Œ ìƒì„±\n",
    "    model_robust.fc = nn.Linear(512, 2)\n",
    "    \n",
    "    model_freq = models.resnet18(pretrained=False)\n",
    "    model_freq.fc = nn.Linear(512, 2)\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê²½ë¡œ/ì´ë¦„ ìœ ì—°í•˜ê²Œ)\n",
    "    path_robust = f\"model_ram_ROBUST_{tag}.pth\"\n",
    "    if not os.path.exists(path_robust): path_robust = \"model_ram_ROBUST.pth\" # Fallback\n",
    "    \n",
    "    path_freq = f\"model_ram_FREQ_{tag}.pth\"\n",
    "    if not os.path.exists(path_freq): path_freq = \"model_ram_FREQ.pth\" # Fallback\n",
    "    \n",
    "    print(f\"   -> Loading ROBUST: {path_robust}\")\n",
    "    if os.path.exists(path_robust):\n",
    "        model_robust.load_state_dict(torch.load(path_robust, map_location=DEVICE))\n",
    "    else:\n",
    "        print(\"[Warning] ROBUST ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ì•™ìƒë¸” ë¶ˆê°€.\")\n",
    "        return\n",
    "\n",
    "    print(f\"   -> Loading FREQ:   {path_freq}\")\n",
    "    if os.path.exists(path_freq):\n",
    "        model_freq.load_state_dict(torch.load(path_freq, map_location=DEVICE))\n",
    "    else:\n",
    "        print(\"[Warning] FREQ ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! ì•™ìƒë¸” ë¶ˆê°€.\")\n",
    "        return\n",
    "\n",
    "    model_robust = model_robust.to(DEVICE).eval()\n",
    "    model_freq = model_freq.to(DEVICE).eval()\n",
    "    \n",
    "    # 4. Inference loop\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    print(\"   -> Starting Ensemble Inference...\")\n",
    "    with torch.no_grad():\n",
    "        # zipìœ¼ë¡œ ë‘ ë¡œë”ë¥¼ ë™ì‹œì— ìˆœíšŒ\n",
    "        for (img_r, lbl), (img_f, _) in tqdm(zip(loader_robust, loader_freq), total=len(loader_robust)):\n",
    "            img_r, img_f = img_r.to(DEVICE), img_f.to(DEVICE)\n",
    "            \n",
    "            # Softmax Probability ê³„ì‚°\n",
    "            prob_r = torch.softmax(model_robust(img_r), dim=1)\n",
    "            prob_f = torch.softmax(model_freq(img_f), dim=1)\n",
    "            \n",
    "            # Fusion (Soft Voting - í‰ê· )\n",
    "            final_prob = (prob_r + prob_f) / 2\n",
    "            \n",
    "            # ìµœì¢… ì˜ˆì¸¡\n",
    "            preds = torch.argmax(final_prob, 1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(lbl.numpy())\n",
    "            \n",
    "    # 5. Analysis\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    print(f\"\\nâœ¨ Final Fusion Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬ í…ìŠ¤íŠ¸ ì¶œë ¥\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(\"-\" * 40)\n",
    "    print(f\" Confusion Matrix:\")\n",
    "    print(f\"               [Pred: Real] | [Pred: Fake]\")\n",
    "    print(f\" [Act: Real]      {tn:^10} | {fp:^10}\")\n",
    "    print(f\" [Act: Fake]      {fn:^10} | {tp:^10}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ì •ë°€ë„/ì¬í˜„ìœ¨ (ë¶„ì„ìš© ë©˜íŠ¸ ìƒì„±ì— í•„ìš”)\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    print(f\" Precision (ê°€ì§œë¼ê³  í–ˆì„ë•Œ ì§„ì§œ ê°€ì§œì¸ ë¹„ìœ¨): {precision:.4f}\")\n",
    "    print(f\" Recall    (ì‹¤ì œ ê°€ì§œ ì¤‘ ì°¾ì•„ë‚¸ ë¹„ìœ¨)        : {recall:.4f}\")\n",
    "    \n",
    "    # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "    plot_confusion_matrix(cm, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f9e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_fusion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccd22ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Finding Best Threshold for [Fusion Ensemble]...\n",
      "Loading data from ../datasets/deepfake_60k_cropped_224px into RAM...\n",
      "Scanning: /home/dongwook/workspace/datasets/deepfake_60k_cropped_224px\n",
      " - Real: Real\n",
      " - Fake: Fake\n",
      " - Total Images Found: 59949\n",
      " - Sampling 10000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Complete! 10000 images. RAM: 1.40 GB\n",
      "   -> Loading ROBUST and FREQ models...\n",
      "   -> Calculating Fusion Probabilities...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:09<00:00, 16.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Fusion Ensemble] Threshold Search Result:\n",
      "\n",
      "âœ¨ Best Accuracy: 0.5820 (Threshold: 0.31)\n",
      "------------------------------\n",
      " Confusion Matrix (Thresh 0.31):\n",
      "[[2373 2574]\n",
      " [1606 3447]]\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def find_best_threshold(use_fusion=False):\n",
    "    mode_str = \"Fusion Ensemble\" if use_fusion else \"Single Model (ROBUST)\"\n",
    "    print(f\"\\nğŸ” Finding Best Threshold for [{mode_str}]...\")\n",
    "    \n",
    "    # 1. Test Data Load\n",
    "    test_x, test_y = load_data_to_ram(TEST_DATA_ROOT, IMG_SIZE, TEST_N_SAMPLES)\n",
    "    if test_x is None: return\n",
    "\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    # 2. Inference\n",
    "    if use_fusion:\n",
    "        print(\"   -> Loading ROBUST and FREQ models...\")\n",
    "        model_rgb = models.resnet18(weights=None); model_rgb.fc = nn.Linear(512, 2)\n",
    "        model_freq = models.resnet18(weights=None); model_freq.fc = nn.Linear(512, 2)\n",
    "        \n",
    "        path_rgb, path_freq = \"model_ram_ROBUST.pth\", \"model_ram_FREQ.pth\"\n",
    "        \n",
    "        if os.path.exists(path_rgb) and os.path.exists(path_freq):\n",
    "            model_rgb.load_state_dict(torch.load(path_rgb, map_location=DEVICE))\n",
    "            model_freq.load_state_dict(torch.load(path_freq, map_location=DEVICE))\n",
    "        else:\n",
    "            print(\"[Error] ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•˜ì„¸ìš”.\")\n",
    "            return\n",
    "\n",
    "        model_rgb.to(DEVICE).eval()\n",
    "        model_freq.to(DEVICE).eval()\n",
    "        \n",
    "        ds_rgb = RAMDataset(test_x, test_y, 'ROBUST', is_train=False)\n",
    "        loader_rgb = DataLoader(ds_rgb, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        ds_freq = RAMDataset(test_x, test_y, 'FREQ', is_train=False)\n",
    "        loader_freq = DataLoader(ds_freq, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(\"   -> Calculating Fusion Probabilities...\")\n",
    "        with torch.no_grad():\n",
    "             for (img_r, lbl), (img_f, _) in tqdm(zip(loader_rgb, loader_freq), total=len(loader_rgb)):\n",
    "                img_r, img_f = img_r.to(DEVICE), img_f.to(DEVICE)\n",
    "                prob_r = torch.softmax(model_rgb(img_r), dim=1)\n",
    "                prob_f = torch.softmax(model_freq(img_f), dim=1)\n",
    "                avg_prob = (prob_r + prob_f) / 2\n",
    "                all_probs.extend(avg_prob[:, 1].cpu().numpy())\n",
    "                all_labels.extend(lbl.numpy())\n",
    "    else:\n",
    "        print(\"   -> Loading single ROBUST model...\")\n",
    "        model = models.resnet18(weights=None); model.fc = nn.Linear(512, 2)\n",
    "        if os.path.exists(\"model_ram_ROBUST.pth\"):\n",
    "            model.load_state_dict(torch.load(\"model_ram_ROBUST.pth\", map_location=DEVICE))\n",
    "        else:\n",
    "            print(\"[Error] model_ram_ROBUST.pth íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        model.to(DEVICE).eval()\n",
    "        \n",
    "        ds = RAMDataset(test_x, test_y, 'ROBUST', is_train=False)\n",
    "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "        \n",
    "        print(\"   -> Calculating Probabilities...\")\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(loader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                probs = torch.softmax(model(inputs), dim=1)[:, 1]\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "\n",
    "    # 3. Threshold Search\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    \n",
    "    best_acc = 0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    print(f\"\\n[{mode_str}] Threshold Search Result:\")\n",
    "    for thresh in np.arange(0.1, 0.91, 0.01):\n",
    "        preds = (all_probs >= thresh).astype(int)\n",
    "        acc = accuracy_score(all_labels, preds)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_thresh = thresh\n",
    "            \n",
    "    print(f\"\\nâœ¨ Best Accuracy: {best_acc:.4f} (Threshold: {best_thresh:.2f})\")\n",
    "    \n",
    "    final_preds = (all_probs >= best_thresh).astype(int)\n",
    "    cm = confusion_matrix(all_labels, final_preds)\n",
    "    print(\"-\" * 30)\n",
    "    print(f\" Confusion Matrix (Thresh {best_thresh:.2f}):\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 30)\n",
    "find_best_threshold(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7d680df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ [FINAL STRATEGY] Domain Adaptation (Kaggle + Wiki 50%)\n",
      "1ï¸âƒ£ Loading Source (Kaggle)...\n",
      "2ï¸âƒ£ Loading Target (Wiki)...\n",
      "   -> Wiki Split: Train(5000) / Evaluation(5000)\n",
      "3ï¸âƒ£ Total Training Set: 65000 images\n",
      "4ï¸âƒ£ Start Adaptation Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 90\u001b[39m\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mprint\u001b[39m(cm)\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[43mrun_domain_adaptation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mrun_domain_adaptation\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(EPOCHS):\n\u001b[32m     51\u001b[39m     model.train()\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEp \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Mixup ì‚¬ìš©\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/CDFSL/venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/CDFSL/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/CDFSL/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/CDFSL/venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mRAMDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Augmentation (Train)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exp_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mROBUST\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mFREQ\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     img_np = \u001b[43mstrong_aug\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Freq Conversion\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exp_name == \u001b[33m'\u001b[39m\u001b[33mFREQ\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mstrong_aug\u001b[39m\u001b[34m(img_np)\u001b[39m\n\u001b[32m     10\u001b[39m     img_np = cv2.GaussianBlur(img_np, (\u001b[32m5\u001b[39m, \u001b[32m5\u001b[39m), \u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m random.random() < \u001b[32m0.3\u001b[39m: \u001b[38;5;66;03m# Noise\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     noise = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_np\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m.astype(np.uint8)\n\u001b[32m     14\u001b[39m     img_np = np.clip(img_np.astype(np.int16) + noise, \u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m).astype(np.uint8)\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m img_np\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def run_domain_adaptation():\n",
    "    print(\"\\nğŸš€ [FINAL STRATEGY] Domain Adaptation (Kaggle + Wiki 50%)\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    print(\"1ï¸âƒ£ Loading Source (Kaggle)...\")\n",
    "    kaggle_x, kaggle_y = train_x, train_y\n",
    "    print(\"2ï¸âƒ£ Loading Target (Wiki)...\")\n",
    "    wiki_x, wiki_y = test_x, test_y\n",
    "    \n",
    "    if kaggle_x is None or wiki_x is None: return\n",
    "\n",
    "    # 2. Wiki ë°ì´í„° ë°˜ìœ¼ë¡œ ìª¼ê°œê¸° (ì„ì–´ì„œ)\n",
    "    total_wiki = len(wiki_x)\n",
    "    indices = torch.randperm(total_wiki)\n",
    "    split_idx = int(total_wiki * 0.5) # 50%ë§Œ í•™ìŠµì— ì‚¬ìš©\n",
    "    \n",
    "    wiki_train_x = wiki_x[indices[:split_idx]]\n",
    "    wiki_train_y = wiki_y[indices[:split_idx]]\n",
    "    \n",
    "    wiki_test_x = wiki_x[indices[split_idx:]]\n",
    "    wiki_test_y = wiki_y[indices[split_idx:]]\n",
    "    \n",
    "    print(f\"   -> Wiki Split: Train({len(wiki_train_x)}) / Evaluation({len(wiki_test_x)})\")\n",
    "\n",
    "    # 3. í•™ìŠµ ë°ì´í„° í•©ì¹˜ê¸° (Kaggle + Wiki ì ˆë°˜)\n",
    "    final_train_x = torch.cat([kaggle_x, wiki_train_x], dim=0)\n",
    "    final_train_y = torch.cat([kaggle_y, wiki_train_y], dim=0)\n",
    "    \n",
    "    print(f\"3ï¸âƒ£ Total Training Set: {len(final_train_x)} images\")\n",
    "\n",
    "    # 4. Dataset & Loader\n",
    "    train_ds = RAMDataset(final_train_x, final_train_y, EXP_NAME, is_train=True)\n",
    "    test_ds = RAMDataset(wiki_test_x, wiki_test_y, EXP_NAME, is_train=False) # ì˜¤ì§ Wikiì˜ ë‚˜ë¨¸ì§€ ì ˆë°˜ìœ¼ë¡œë§Œ í‰ê°€\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 5. Model (Pretrained)\n",
    "    model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    # Fine-tuningì´ë¯€ë¡œ LRì„ ë‚®ê²Œ ì„¤ì •\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005) \n",
    "    \n",
    "    best_acc = 0.0\n",
    "    print(\"4ï¸âƒ£ Start Adaptation Training...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Mixup ì‚¬ìš©\n",
    "            inputs, ta, tb, lam = mixup_data(inputs, labels, alpha=0.4)\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, ta, tb, lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # í‰ê°€\n",
    "        model.eval()\n",
    "        preds, acts = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "                acts.extend(labels.numpy())\n",
    "        \n",
    "        acc = accuracy_score(acts, preds)\n",
    "        print(f\"Epoch {epoch+1} | Adapted Test Acc: {acc:.4f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"model_ram_ADAPTED.pth\")\n",
    "            \n",
    "    print(f\"\\nâœ¨ Final Adapted Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬ ì¶œë ¥\n",
    "    cm = confusion_matrix(acts, preds)\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "run_domain_adaptation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd7b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸš€ [FINAL STRATEGY] Domain Adaptation (Kaggle + Wiki 50%)\n",
      "1ï¸âƒ£ Loading Source (Kaggle)...\n",
      "2ï¸âƒ£ Loading Target (Wiki)...\n",
      "   -> Wiki Split: Train(1000) / Evaluation(9000)\n",
      "3ï¸âƒ£ Total Training Set: 1000 images\n",
      "4ï¸âƒ£ Start Adaptation Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongwook/workspace/CDFSL/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dongwook/workspace/CDFSL/venv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Adapted Test Acc: 0.5851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Adapted Test Acc: 0.5883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Adapted Test Acc: 0.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Adapted Test Acc: 0.5856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Adapted Test Acc: 0.5852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Adapted Test Acc: 0.5841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Adapted Test Acc: 0.5828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Adapted Test Acc: 0.5826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Adapted Test Acc: 0.5809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Adapted Test Acc: 0.5811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | Adapted Test Acc: 0.5822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | Adapted Test Acc: 0.5778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 | Adapted Test Acc: 0.5786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 | Adapted Test Acc: 0.5756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 | Adapted Test Acc: 0.5784\n",
      "\n",
      "âœ¨ Final Adapted Accuracy: 0.5883\n",
      "------------------------------\n",
      "Confusion Matrix:\n",
      "[[2726 1809]\n",
      " [1985 2480]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def run_domain_adaptation2():\n",
    "    print(\"\\nğŸš€ [FINAL STRATEGY] Domain Adaptation (Kaggle + Wiki 50%)\")\n",
    "    \n",
    "    # 1. ë°ì´í„° ë¡œë“œ\n",
    "    print(\"1ï¸âƒ£ Loading Source (Kaggle)...\")\n",
    "    kaggle_x, kaggle_y = train_x, train_y\n",
    "    print(\"2ï¸âƒ£ Loading Target (Wiki)...\")\n",
    "    wiki_x, wiki_y = test_x, test_y\n",
    "    \n",
    "    if kaggle_x is None or wiki_x is None: return\n",
    "\n",
    "    # 2. Wiki ë°ì´í„° ì¡°ê¸ˆë§Œ ìª¼ê°œê¸° (ì„ì–´ì„œ)\n",
    "    total_wiki = len(wiki_x)\n",
    "    indices = torch.randperm(total_wiki)\n",
    "    split_idx = int(total_wiki * 0.1) # 1000ê°œë§Œ í•™ìŠµì— ì‚¬ìš©\n",
    "    \n",
    "    wiki_train_x = wiki_x[indices[:split_idx]]\n",
    "    wiki_train_y = wiki_y[indices[:split_idx]]\n",
    "    \n",
    "    wiki_test_x = wiki_x[indices[split_idx:]]\n",
    "    wiki_test_y = wiki_y[indices[split_idx:]]\n",
    "    \n",
    "    print(f\"   -> Wiki Split: Train({len(wiki_train_x)}) / Evaluation({len(wiki_test_x)})\")\n",
    "\n",
    "    # 3. í•™ìŠµ ë°ì´í„° í•©ì¹˜ê¸° (Kaggle + Wiki ì ˆë°˜)\n",
    "    final_train_x = wiki_train_x\n",
    "    final_train_y = wiki_test_y\n",
    "    \n",
    "    print(f\"3ï¸âƒ£ Total Training Set: {len(final_train_x)} images\")\n",
    "\n",
    "    # 4. Dataset & Loader\n",
    "    train_ds = RAMDataset(final_train_x, final_train_y, EXP_NAME, is_train=True)\n",
    "    test_ds = RAMDataset(wiki_test_x, wiki_test_y, EXP_NAME, is_train=False) # ì˜¤ì§ Wikiì˜ ë‚˜ë¨¸ì§€ ì ˆë°˜ìœ¼ë¡œë§Œ í‰ê°€\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # 5. Load Models\n",
    "    model = models.resnet18(pretrained=False) # êµ¬ì¡°ë§Œ ìƒì„±\n",
    "    model.fc = nn.Linear(512, 2)\n",
    "    \n",
    "    # ëª¨ë¸ íŒŒì¼ ì°¾ê¸° (ê²½ë¡œ/ì´ë¦„ ìœ ì—°í•˜ê²Œ)\n",
    "    path = f\"model_ram_ROBUST.pth\"\n",
    "    model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
    "\n",
    "    # Fine-tuningì´ë¯€ë¡œ LRì„ ë‚®ê²Œ ì„¤ì •\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.000001) \n",
    "    \n",
    "    best_acc = 0.0\n",
    "    print(\"4ï¸âƒ£ Start Adaptation Training...\")\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Ep {epoch+1}\", leave=False):\n",
    "            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            # Mixup ì‚¬ìš©\n",
    "            inputs, ta, tb, lam = mixup_data(inputs, labels, alpha=0.25)\n",
    "            outputs = model(inputs)\n",
    "            loss = mixup_criterion(criterion, outputs, ta, tb, lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # í‰ê°€\n",
    "        model.eval()\n",
    "        preds, acts = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                outputs = model(inputs)\n",
    "                preds.extend(torch.argmax(outputs, 1).cpu().numpy())\n",
    "                acts.extend(labels.numpy())\n",
    "        \n",
    "        acc = accuracy_score(acts, preds)\n",
    "        print(f\"Epoch {epoch+1} | Adapted Test Acc: {acc:.4f}\")\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), \"model_ram_ADAPTED2.pth\")\n",
    "            \n",
    "    print(f\"\\nâœ¨ Final Adapted Accuracy: {best_acc:.4f}\")\n",
    "    \n",
    "    # í˜¼ë™ í–‰ë ¬ ì¶œë ¥\n",
    "    cm = confusion_matrix(acts, preds)\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "run_domain_adaptation2()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

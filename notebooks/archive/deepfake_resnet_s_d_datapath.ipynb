{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/sazaqa0901/ML_test/blob/main/deepfake_resnet_s.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"pYonvVA7wjeR"},"source":["resnet50\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuQ-92WQx0En","outputId":"cf6f290a-1a89-40e7-cb2e-e0e74971d9e6","executionInfo":{"status":"ok","timestamp":1763904982773,"user_tz":-540,"elapsed":37416,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import zipfile\n","zip_file_name = '/content/drive/MyDrive/Dataset.zip'\n","extraction_dir = '/content/dataset'\n","with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n","  zip_ref.extractall(extraction_dir)"],"metadata":{"id":"frg-vDR7x1IN","executionInfo":{"status":"ok","timestamp":1763905037887,"user_tz":-540,"elapsed":55123,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EJWtewChprMW","executionInfo":{"status":"ok","timestamp":1763905050915,"user_tz":-540,"elapsed":13014,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from PIL import Image\n","\n","import os\n","import glob\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","import zipfile\n","import time\n","import copy\n","\n","\n","# import h5py\n","# import cv2"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"WfsznOINwjeX","executionInfo":{"status":"ok","timestamp":1763905050936,"user_tz":-540,"elapsed":8,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["# 설정값\n","IMG_SIZE = 224\n","EPOCHS = 30\n","BATCH_SIZE = 64\n","NUM_SAMPLES = 20000\n","LEARNING_RATE = 1e-4\n","PATIENCE = 5"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"ak9tUFkiwjeY","executionInfo":{"status":"ok","timestamp":1763905050960,"user_tz":-540,"elapsed":19,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["# 이미지 전처리 클래스\n","# 비율 유지하며 resize 후 패딩 추가(0으로)\n","class ResizeWithPad:\n","    def __init__(self, target_size):\n","        self.target_size = target_size\n","\n","    def __call__(self, img):\n","        w, h = img.size\n","\n","        scale = self.target_size / max(w, h)\n","\n","        new_w = int(w * scale)\n","        new_h = int(h * scale)\n","\n","        # 비율 유지하며 리사이즈\n","        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n","\n","        # 검은색 캔버스 생성\n","        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n","\n","        # 캔버스 중앙에 리사이즈된 이미지 배치\n","        pad_x = (self.target_size - new_w) // 2\n","        pad_y = (self.target_size - new_h) // 2\n","\n","        canvas.paste(resized_img, (pad_x, pad_y))\n","\n","        return canvas"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"PpzKAxb3-neB","executionInfo":{"status":"ok","timestamp":1763905050975,"user_tz":-540,"elapsed":11,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["# 데이터셋\n","# 디스크의 이미지 경로 리스트를 받아,\n","# 배치 생성 시점에만 이미지를 읽어옴\n","class DeepfakeDataset(Dataset):\n","    def __init__(self, paths, labels, transform=None):\n","        self.paths = paths\n","        self.labels = labels\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","    def __getitem__(self, idx):\n","        # 디스크에서 이미지 경로로 이미지 로드 (PIL)\n","        img_path = self.paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","\n","        # 전처리 적용\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # 라벨을 float 텐서로 변환 (BCEWithLogitsLoss용)\n","        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n","\n","        return image, label.unsqueeze(0) # (1,) 형태로 반환\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LtDP02qNtpRB","executionInfo":{"status":"ok","timestamp":1763905050995,"user_tz":-540,"elapsed":16,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["class AlexNetLike(nn.Module):\n","    def __init__(self, num_classes=1):\n","        super(AlexNetLike, self).__init__()\n","        self.features = nn.Sequential(\n","            # Conv 1\n","            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), # 224 -> 55\n","            nn.BatchNorm2d(96),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2), # 55 -> 27\n","\n","            # Conv 2\n","            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 27 -> 27\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2), # 27 -> 13\n","\n","            # Conv 3\n","            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 13 -> 13\n","            nn.BatchNorm2d(384),\n","            nn.ReLU(inplace=True),\n","\n","            # Conv 4\n","            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 13 -> 13\n","            nn.BatchNorm2d(384),\n","            nn.ReLU(inplace=True),\n","\n","            # Conv 5\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 13 -> 13\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=3, stride=2), # 13 -> 6\n","        )\n","        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n","        self.classifier = nn.Sequential(\n","            nn.Dropout(0.5),\n","            nn.Linear(256 * 6 * 6, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Dropout(0.5),\n","            nn.Linear(1024, 1024),\n","            nn.ReLU(inplace=True),\n","            nn.Linear(1024, num_classes) # num_classes=1\n","        )\n","    def forward(self, x):\n","        x = self.features(x); x = self.avgpool(x)\n","        x = torch.flatten(x, 1); x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"tqWWMXSPwjea","executionInfo":{"status":"ok","timestamp":1763905051022,"user_tz":-540,"elapsed":23,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["def get_model(model_name, device, use_pretrained=True):\n","\n","    model = None\n","    num_classes = 1 #이진 분류 (Real/Fake)\n","\n","    weights = models.ResNet50_Weights.IMAGENET1K_V1 if use_pretrained else None\n","\n","    print(f\"Loading {model_name} architecture...\")\n","    if use_pretrained:\n","        print(\"  Using ImageNet Pre-trained Weights.\")\n","    else:\n","        print(\"  Training FROM SCRATCH (No Pre-trained Weights).\")\n","\n","    if model_name.lower() == 'alexnet':\n","        # 직접(동욱) 짠 AlexNet\n","        model = AlexNetLike(num_classes=num_classes)\n","        print(\"  Note: Using custom AlexNetLike, ignoring 'use_pretrained'.\")\n","\n","    elif model_name.lower() == 'vgg16':\n","        model = models.vgg16(weights=weights if model_name.lower() == 'vgg16' else None)\n","        num_ftrs = model.classifier[6].in_features\n","        model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n","\n","    elif model_name.lower() == 'googlenet':\n","        model = models.googlenet(weights=weights if model_name.lower() == 'googlenet' else None, num_classes=num_classes, aux_logits=False)\n","\n","    elif model_name.lower() == 'resnet50':\n","        model = models.resnet50(weights=weights)\n","        num_ftrs = model.fc.in_features\n","        model.fc = nn.Sequential(\n","        nn.Dropout(p=0.3),\n","        nn.Linear(num_ftrs, num_classes)\n","        )\n","\n","    else:\n","        raise ValueError(f\"Unknown model name: {model_name}. Choose from 'alexnet', 'vgg16', 'googlenet', 'resnet50'\")\n","\n","    return model.to(device)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"H56_ULwNi61i","executionInfo":{"status":"ok","timestamp":1763905051043,"user_tz":-540,"elapsed":22,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["# 학습\n","def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n","    print(\"=== 학습 시작 ===\")\n","\n","    best_val_loss = float('inf')\n","    best_model_weights = None\n","    epochs_no_improve = 0\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()\n","\n","        # --- 훈련 ---\n","        model.train()\n","        running_loss = 0.0\n","        correct_train = 0\n","        total_train = 0\n","\n","        # tqdm으로 진행 상황 표시\n","        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n","\n","        for images, labels in train_pbar:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            # 순전파\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            # 역전파 및 최적화\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * images.size(0)\n","\n","            # 정확도 계산\n","            preds = torch.sigmoid(outputs) > 0.5\n","            total_train += labels.size(0)\n","            correct_train += (preds == labels).sum().item()\n","\n","            train_pbar.set_postfix({'loss': loss.item()})\n","\n","        epoch_train_loss = running_loss / len(train_loader.dataset)\n","        epoch_train_acc = correct_train / total_train\n","\n","        # --- 검증 ---\n","        model.eval()\n","        running_val_loss = 0.0\n","        correct_val = 0\n","        total_val = 0\n","\n","        with torch.no_grad():\n","            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n","            for images, labels in val_pbar:\n","                images, labels = images.to(device), labels.to(device)\n","\n","                outputs = model(images)\n","                loss = criterion(outputs, labels)\n","\n","                running_val_loss += loss.item() * images.size(0)\n","\n","                preds = torch.sigmoid(outputs) > 0.5\n","                total_val += labels.size(0)\n","                correct_val += (preds == labels).sum().item()\n","\n","        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n","        epoch_val_acc = correct_val / total_val\n","\n","        elapsed_time = time.time() - start_time\n","\n","        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.0f}s - \"\n","              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n","              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n","\n","        # --- Early Stopping 및 Best Model 저장 ---\n","        if epoch_val_loss < best_val_loss:\n","            print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {epoch_val_loss:.4f}). Saving model...\")\n","            best_val_loss = epoch_val_loss\n","            best_model_weights = copy.deepcopy(model.state_dict())\n","            torch.save(best_model_weights, MODEL_SAVE_PATH)\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n","\n","        if epochs_no_improve >= patience:\n","            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n","            break\n","\n","        scheduler.step(epoch_val_loss)\n","\n","    print(\"=== 학습 완료 ===\")\n","    model.load_state_dict(best_model_weights)\n","    return model"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"df3H5EVywjeb","executionInfo":{"status":"ok","timestamp":1763905051069,"user_tz":-540,"elapsed":21,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"outputs":[],"source":["# 평가\n","def evaluate_model(model, test_loader, criterion, device):\n","    model.eval()\n","    running_test_loss = 0.0\n","    correct_test = 0\n","    total_test = 0\n","\n","    print(\"\\n=== 테스트셋 평가 시작 ===\")\n","    with torch.no_grad():\n","        test_pbar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n","        for images, labels in test_pbar:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","            running_test_loss += loss.item() * images.size(0)\n","\n","            preds = torch.sigmoid(outputs) > 0.5\n","            total_test += labels.size(0)\n","            correct_test += (preds == labels).sum().item()\n","\n","    test_loss = running_test_loss / len(test_loader.dataset)\n","    test_acc = correct_test / total_test\n","\n","    print(f\"===== 최종 테스트 결과 =====\")\n","    print(f\"  Test Loss: {test_loss:.4f}\")\n","    print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")"]},{"cell_type":"code","source":["# 데이터 path 만들기\n","def load_and_sample(real_dir, fake_dir, n_samples=None):\n","    real_paths = []\n","    fake_paths = []\n","\n","    real_paths.extend(glob.glob(os.path.join(real_dir, \"*.*\")))\n","    fake_paths.extend(glob.glob(os.path.join(fake_dir, \"*.*\")))\n","\n","    paths = real_paths + fake_paths\n","    labels = [0] * len(real_paths) + [1] * len(fake_paths)\n","    total = len(paths)\n","\n","    print(f\"경로: {os.path.dirname(real_dir)}\")\n","    print(f\"개수: {total}장 (Real: {len(real_paths)}, Fake: {len(fake_paths)})\")\n","\n","    if n_samples is None or n_samples >= total:\n","        print(f\"  - 전체 데이터 사용 ({total}장)\")\n","        # 셔플만 수행\n","        combined = list(zip(paths, labels))\n","        np.random.shuffle(combined)\n","        paths[:], labels[:] = zip(*combined)\n","        return paths, labels\n","\n","    else:\n","        print(f\"  - 샘플링: {n_samples}장 선택 (Stratified)\")\n","\n","        sampled_paths, _, sampled_labels, _ = train_test_split(\n","            paths, labels,\n","            train_size=n_samples,\n","            stratify=labels,\n","            random_state=42\n","        )\n","        return sampled_paths, sampled_labels"],"metadata":{"id":"iTWoO8jEwYKR","executionInfo":{"status":"ok","timestamp":1763905051087,"user_tz":-540,"elapsed":13,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# 메인 실행\n","start_time = time.time()\n","\n","# GPU 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 데이터 경로 및 라벨 수집\n","base_path = \"/content/dataset/Dataset/\"\n","train_path = base_path + 'Train'\n","validdation_path = base_path + 'Validation'\n","test_path = base_path + 'Test'\n","MODEL_SAVE_PATH = \"/content/drive/MyDrive/deepfake_model.pth\"\n","\n","NUM_TRAIN_SAMPLES = 14000\n","NUM_VAL_SAMPLES = 4000\n","NUM_TEST_SAMPLES = 2000    # None: 전체 사용\n","\n","print(\"\\n=== [Train Set] 준비 ===\")\n","train_paths, train_labels = load_and_sample(\n","    os.path.join(train_path, 'Real'),\n","    os.path.join(train_path, 'Fake'),\n","    NUM_TRAIN_SAMPLES\n",")\n","\n","print(\"\\n=== [Validation Set] 준비 ===\")\n","val_paths, val_labels = load_and_sample(\n","    os.path.join(validdation_path, 'Real'),\n","    os.path.join(validdation_path, 'Fake'),\n","    NUM_VAL_SAMPLES\n",")\n","\n","print(\"\\n=== [Test Set] 준비 ===\")\n","test_paths, test_labels = load_and_sample(\n","    os.path.join(test_path, 'Real'),\n","    os.path.join(test_path, 'Fake'),\n","    NUM_TEST_SAMPLES\n",")\n","\n","#target_labels = list(set((train_labels + val_labels + test_labels)))\n","\n","# 4. 결과 확인\n","print(f\"\\n최종 데이터셋 구성:\")\n","print(f\"  Train: {len(train_paths)}장\")\n","print(f\"  Val  : {len(val_paths)}장\")\n","print(f\"  Test : {len(test_paths)}장\")"],"metadata":{"id":"9kzTIAFLwhv7","executionInfo":{"status":"ok","timestamp":1763905051604,"user_tz":-540,"elapsed":440,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2308bdd7-3e72-4286-f59b-aaa0979fdec1"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","=== [Train Set] 준비 ===\n","경로: /content/dataset/Dataset/Train\n","개수: 140002장 (Real: 70001, Fake: 70001)\n","  - 샘플링: 14000장 선택 (Stratified)\n","\n","=== [Validation Set] 준비 ===\n","경로: /content/dataset/Dataset/Validation\n","개수: 39428장 (Real: 19787, Fake: 19641)\n","  - 샘플링: 4000장 선택 (Stratified)\n","\n","=== [Test Set] 준비 ===\n","경로: /content/dataset/Dataset/Test\n","개수: 10905장 (Real: 5413, Fake: 5492)\n","  - 샘플링: 2000장 선택 (Stratified)\n","\n","최종 데이터셋 구성:\n","  Train: 14000장\n","  Val  : 4000장\n","  Test : 2000장\n"]}]},{"cell_type":"code","source":["#데이터 로더\n","train_transform = transforms.Compose([\n","    ResizeWithPad(IMG_SIZE),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","    transforms.RandomRotation(degrees=5),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) ])\n","val_test_transform = transforms.Compose([\n","    ResizeWithPad(IMG_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","    ])\n","train_dataset = DeepfakeDataset(train_paths, train_labels, transform=train_transform)\n","val_dataset = DeepfakeDataset(val_paths, val_labels, transform=val_test_transform)\n","test_dataset = DeepfakeDataset(test_paths, test_labels, transform=val_test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","\n","print(\"데이터 로더 생성 완료.\")"],"metadata":{"id":"RjrkSyrdw4VK","executionInfo":{"status":"ok","timestamp":1763905051634,"user_tz":-540,"elapsed":26,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b1c33fb-0bd9-4906-d4de-23f101bb5d7b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 로더 생성 완료.\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"wRUieuirwjed","executionInfo":{"status":"ok","timestamp":1763906167842,"user_tz":-540,"elapsed":44,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ea6ad7d4-8feb-4f94-d08b-6e6f4d232af0"},"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 로더 생성 완료.\n"]}],"source":["\n","# 전처리 및 데이터 로더 정의\n","train_transform = transforms.Compose([\n","    ResizeWithPad(IMG_SIZE),\n","    transforms.RandomHorizontalFlip(), # 데이터 증강\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # -1 ~ 1 정규화\n","])\n","\n","val_test_transform = transforms.Compose([\n","    ResizeWithPad(IMG_SIZE),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","])\n","\n","train_dataset = DeepfakeDataset(train_paths, train_labels, transform=train_transform)\n","val_dataset = DeepfakeDataset(val_paths, val_labels, transform=val_test_transform)\n","test_dataset = DeepfakeDataset(test_paths, test_labels, transform=val_test_transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","\n","print(\"데이터 로더 생성 완료.\")\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"T4_5lk7hwjed","executionInfo":{"status":"ok","timestamp":1763910057618,"user_tz":-540,"elapsed":3887401,"user":{"displayName":"김윤숙","userId":"06661495915909363949"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"df7575ee-c2dd-49f2-8b9f-039c50fbb878"},"outputs":[{"output_type":"stream","name":"stdout","text":["데이터 로더(디스크 기반)를 사용하여 학습을 시작합니다.\n","Loading resnet50 architecture...\n","  Using ImageNet Pre-trained Weights.\n","Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 97.8M/97.8M [00:00<00:00, 203MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["=== 학습 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30 - 138s - Train Loss: 0.6397, Train Acc: 0.6476 - Val Loss: 0.5787, Val Acc: 0.7548\n","  Validation loss decreased (inf --> 0.5787). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 2/30 - 130s - Train Loss: 0.5694, Train Acc: 0.7364 - Val Loss: 0.5339, Val Acc: 0.7675\n","  Validation loss decreased (0.5787 --> 0.5339). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 3/30 - 129s - Train Loss: 0.5307, Train Acc: 0.7606 - Val Loss: 0.5012, Val Acc: 0.7853\n","  Validation loss decreased (0.5339 --> 0.5012). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 4/30 - 128s - Train Loss: 0.5051, Train Acc: 0.7703 - Val Loss: 0.4829, Val Acc: 0.7925\n","  Validation loss decreased (0.5012 --> 0.4829). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 5/30 - 128s - Train Loss: 0.4897, Train Acc: 0.7768 - Val Loss: 0.4745, Val Acc: 0.7880\n","  Validation loss decreased (0.4829 --> 0.4745). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 6/30 - 128s - Train Loss: 0.4797, Train Acc: 0.7808 - Val Loss: 0.4643, Val Acc: 0.7917\n","  Validation loss decreased (0.4745 --> 0.4643). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 7/30 - 128s - Train Loss: 0.4668, Train Acc: 0.7890 - Val Loss: 0.4591, Val Acc: 0.7960\n","  Validation loss decreased (0.4643 --> 0.4591). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 8/30 - 128s - Train Loss: 0.4629, Train Acc: 0.7850 - Val Loss: 0.4524, Val Acc: 0.7963\n","  Validation loss decreased (0.4591 --> 0.4524). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 9/30 - 128s - Train Loss: 0.4539, Train Acc: 0.7900 - Val Loss: 0.4528, Val Acc: 0.7960\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 10/30 - 128s - Train Loss: 0.4503, Train Acc: 0.7910 - Val Loss: 0.4548, Val Acc: 0.7965\n","  Validation loss did not improve. Patience: 2/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 11/30 - 128s - Train Loss: 0.4460, Train Acc: 0.7929 - Val Loss: 0.4468, Val Acc: 0.7947\n","  Validation loss decreased (0.4524 --> 0.4468). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 12/30 - 129s - Train Loss: 0.4452, Train Acc: 0.7926 - Val Loss: 0.4458, Val Acc: 0.7975\n","  Validation loss decreased (0.4468 --> 0.4458). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 13/30 - 129s - Train Loss: 0.4383, Train Acc: 0.7980 - Val Loss: 0.4458, Val Acc: 0.7995\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 14/30 - 129s - Train Loss: 0.4377, Train Acc: 0.8014 - Val Loss: 0.4442, Val Acc: 0.7977\n","  Validation loss decreased (0.4458 --> 0.4442). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 15/30 - 129s - Train Loss: 0.4359, Train Acc: 0.7976 - Val Loss: 0.4395, Val Acc: 0.7985\n","  Validation loss decreased (0.4442 --> 0.4395). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 16/30 - 128s - Train Loss: 0.4326, Train Acc: 0.8016 - Val Loss: 0.4424, Val Acc: 0.7985\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 17/30 - 128s - Train Loss: 0.4297, Train Acc: 0.8008 - Val Loss: 0.4390, Val Acc: 0.7983\n","  Validation loss decreased (0.4395 --> 0.4390). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 18/30 - 129s - Train Loss: 0.4287, Train Acc: 0.8006 - Val Loss: 0.4373, Val Acc: 0.7987\n","  Validation loss decreased (0.4390 --> 0.4373). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 19/30 - 129s - Train Loss: 0.4299, Train Acc: 0.8009 - Val Loss: 0.4411, Val Acc: 0.7990\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 20/30 - 128s - Train Loss: 0.4313, Train Acc: 0.8005 - Val Loss: 0.4362, Val Acc: 0.7960\n","  Validation loss decreased (0.4373 --> 0.4362). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 21/30 - 128s - Train Loss: 0.4241, Train Acc: 0.8035 - Val Loss: 0.4340, Val Acc: 0.7990\n","  Validation loss decreased (0.4362 --> 0.4340). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 22/30 - 129s - Train Loss: 0.4257, Train Acc: 0.8019 - Val Loss: 0.4380, Val Acc: 0.7987\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 23/30 - 128s - Train Loss: 0.4230, Train Acc: 0.8053 - Val Loss: 0.4325, Val Acc: 0.8003\n","  Validation loss decreased (0.4340 --> 0.4325). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 24/30 - 129s - Train Loss: 0.4190, Train Acc: 0.8053 - Val Loss: 0.4335, Val Acc: 0.7985\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 25/30 - 129s - Train Loss: 0.4240, Train Acc: 0.8039 - Val Loss: 0.4327, Val Acc: 0.7995\n","  Validation loss did not improve. Patience: 2/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 26/30 - 129s - Train Loss: 0.4189, Train Acc: 0.8065 - Val Loss: 0.4309, Val Acc: 0.8017\n","  Validation loss decreased (0.4325 --> 0.4309). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 27/30 - 128s - Train Loss: 0.4206, Train Acc: 0.8061 - Val Loss: 0.4301, Val Acc: 0.7987\n","  Validation loss decreased (0.4309 --> 0.4301). Saving model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 28/30 - 129s - Train Loss: 0.4183, Train Acc: 0.8080 - Val Loss: 0.4303, Val Acc: 0.7993\n","  Validation loss did not improve. Patience: 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 29/30 - 128s - Train Loss: 0.4194, Train Acc: 0.8053 - Val Loss: 0.4335, Val Acc: 0.7985\n","  Validation loss did not improve. Patience: 2/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 30/30 - 128s - Train Loss: 0.4193, Train Acc: 0.8064 - Val Loss: 0.4294, Val Acc: 0.7997\n","  Validation loss decreased (0.4301 --> 0.4294). Saving model...\n","=== 학습 완료 ===\n","\n","=== 테스트셋 평가 시작 ===\n"]},{"output_type":"stream","name":"stderr","text":["                                                       "]},{"output_type":"stream","name":"stdout","text":["===== 최종 테스트 결과 =====\n","  Test Loss: 0.5494\n","  Test Accuracy: 72.60%\n","총 실행 시간: 83.44 분\n"]},{"output_type":"stream","name":"stderr","text":["\r"]}],"source":["print(\"데이터 로더(디스크 기반)를 사용하여 학습을 시작합니다.\")\n","\n","#모델, 손실함수, 옵티마이저 정의\n","MODEL_NAME = 'resnet50'\n","model = get_model(MODEL_NAME, device, use_pretrained=True) #use_pretrained 사용\n","\n","# 모든 파라미터 동결\n","for name, param in model.named_parameters():\n","    param.requires_grad = False\n","\n","for param in model.fc.parameters():\n","    param.requires_grad = True\n","\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","    optimizer,\n","    mode='min',\n","    factor=0.1,\n","    patience=3,\n","    #verbose=True\n",")\n","\n","#학습 및 평가\n","# cell 10에서 정의된 train_loader, val_loader를 사용\n","model = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n","\n","evaluate_model(model, test_loader, criterion, device)\n","\n","print(f\"총 실행 시간: {(time.time() - start_time) / 60:.2f} 분\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"history_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.8"}},"nbformat":4,"nbformat_minor":0}
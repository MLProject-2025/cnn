{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3739,
     "status": "ok",
     "timestamp": 1761667257161,
     "user": {
      "displayName": "박동욱",
      "userId": "18205290377464024074"
     },
     "user_tz": -540
    },
    "id": "EJWtewChprMW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "# import h5py\n",
    "# import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정값 \n",
    "\n",
    "IMG_SIZE = 224\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "NUM_SAMPLES = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "PATIENCE = 5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 전처리 클래스\n",
    "# 비율 유지하며 resize 후 패딩 추가(0으로)\n",
    "class ResizeWithPad:\n",
    "    def __init__(self, target_size):\n",
    "        self.target_size = target_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        w, h = img.size\n",
    "        \n",
    "        scale = self.target_size / max(w, h)\n",
    "        \n",
    "        new_w = int(w * scale)\n",
    "        new_h = int(h * scale)\n",
    "        \n",
    "        # 비율 유지하며 리사이즈\n",
    "        resized_img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "        \n",
    "        # 검은색 캔버스 생성\n",
    "        canvas = Image.new(\"RGB\", (self.target_size, self.target_size), (0, 0, 0))\n",
    "        \n",
    "        # 캔버스 중앙에 리사이즈된 이미지 배치\n",
    "        pad_x = (self.target_size - new_w) // 2\n",
    "        pad_y = (self.target_size - new_h) // 2\n",
    "        \n",
    "        canvas.paste(resized_img, (pad_x, pad_y))\n",
    "        \n",
    "        return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1761667257186,
     "user": {
      "displayName": "박동욱",
      "userId": "18205290377464024074"
     },
     "user_tz": -540
    },
    "id": "PpzKAxb3-neB"
   },
   "outputs": [],
   "source": [
    "# 데이터셋\n",
    "# 디스크의 이미지 경로 리스트를 받아,\n",
    "# 배치 생성 시점에만 이미지를 읽어옴\n",
    "class DeepfakeDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 디스크에서 이미지 경로로 이미지 로드 (PIL)\n",
    "        img_path = self.paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # 2. 전처리 적용\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # 3. 라벨을 float 텐서로 변환 (BCEWithLogitsLoss용)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image, label.unsqueeze(0) # (1,) 형태로 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1761667274034,
     "user": {
      "displayName": "박동욱",
      "userId": "18205290377464024074"
     },
     "user_tz": -540
    },
    "id": "LtDP02qNtpRB"
   },
   "outputs": [],
   "source": [
    "class AlexNetLike(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(AlexNetLike, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Conv 1\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=2), # 224 -> 55\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 55 -> 27\n",
    "            \n",
    "            # Conv 2\n",
    "            nn.Conv2d(96, 256, kernel_size=5, padding=2), # 27 -> 27\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 27 -> 13\n",
    "            \n",
    "            # Conv 3\n",
    "            nn.Conv2d(256, 384, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv 4\n",
    "            nn.Conv2d(384, 384, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Conv 5\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1), # 13 -> 13\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2), # 13 -> 6\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256 * 6 * 6, 1024), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, num_classes) # num_classes=1\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x); x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1); x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, device):\n",
    "\n",
    "    model = None\n",
    "    num_classes = 1 # 이진 분류 (Real/Fake)\n",
    "    \n",
    "    print(f\"Loading {model_name} architecture (FROM SCRATCH)...\")\n",
    "\n",
    "    if model_name.lower() == 'alexnet': \n",
    "        # 우리가 직접 짠 AlexNet (Conv 5, FC 3)\n",
    "        model = AlexNetLike(num_classes=num_classes)\n",
    "    \n",
    "    elif model_name.lower() == 'vgg16':\n",
    "        # VGG16 아키텍처를 import (weights=None: 처음부터 학습)\n",
    "        model = models.vgg16(weights=None, num_classes=num_classes)\n",
    "    \n",
    "    elif model_name.lower() == 'googlenet':\n",
    "        # GoogLeNet 아키텍처를 import (weights=None: 처음부터 학습)\n",
    "        model = models.googlenet(weights=None, num_classes=num_classes, aux_logits=False)\n",
    "    \n",
    "    elif model_name.lower() == 'resnet50':\n",
    "        # ResNet50 아키텍처를 import (weights=None: 처음부터 학습)\n",
    "        model = models.resnet50(weights=None, num_classes=num_classes)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}. Choose from 'alexnet, 'vgg16', 'googlenet', 'resnet50'\")\n",
    "        \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 237,
     "status": "ok",
     "timestamp": 1761667274278,
     "user": {
      "displayName": "박동욱",
      "userId": "18205290377464024074"
     },
     "user_tz": -540
    },
    "id": "H56_ULwNi61i"
   },
   "outputs": [],
   "source": [
    "# 학습\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, patience):\n",
    "    print(\"=== 학습 시작 ===\")\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # --- 훈련 ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        # tqdm으로 진행 상황 표시\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False)\n",
    "        \n",
    "        for images, labels in train_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # 순전파\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 역전파 및 최적화\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # 정확도 계산\n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "            \n",
    "            train_pbar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct_train / total_train\n",
    "\n",
    "        # --- 검증 ---\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False)\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                running_val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                preds = torch.sigmoid(outputs) > 0.5\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "        \n",
    "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = correct_val / total_val\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - {elapsed_time:.0f}s - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # --- Early Stopping 및 Best Model 저장 ---\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            print(f\"  Validation loss decreased ({best_val_loss:.4f} --> {epoch_val_loss:.4f}). Saving model...\")\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_weights, MODEL_SAVE_PATH)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"  Validation loss did not improve. Patience: {epochs_no_improve}/{patience}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "            \n",
    "    print(\"=== 학습 완료 ===\")\n",
    "    model.load_state_dict(best_model_weights)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    \n",
    "    print(\"\\n=== 테스트셋 평가 시작 ===\")\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(test_loader, desc=\"[Test]\", leave=False)\n",
    "        for images, labels in test_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_test_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            preds = torch.sigmoid(outputs) > 0.5\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "            \n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    test_acc = correct_test / total_test\n",
    "    \n",
    "    print(f\"===== 최종 테스트 결과 =====\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "이미지 경로 수집 중...\n",
      "총 68479개 이미지 경로 발견.\n",
      "10000개 샘플을 샘플링...\n",
      "샘플링 완료: 10000개 이미지 선택.\n"
     ]
    }
   ],
   "source": [
    "# 메인 실행\n",
    "start_time = time.time()\n",
    "\n",
    "# GPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 3. 데이터 경로 및 라벨 수집\n",
    "Fake_PATH = \"D:/workspace/python/experiments/dataset/univ_ML_basic/deepfake/cropped/fake\"\n",
    "Real_PATH = \"D:/workspace/python/experiments/dataset/univ_ML_basic/deepfake/cropped/real\"\n",
    "MODEL_SAVE_PATH = \"D:/workspace/python/experiments//deepfake_baseline_model.pth\"\n",
    "print(\"이미지 경로 수집 중...\")\n",
    "face_real_dir = os.path.join(Real_PATH)\n",
    "face_fake_dir = os.path.join(Fake_PATH)\n",
    "\n",
    "real_paths = glob.glob(os.path.join(face_real_dir, \"*.*\"))\n",
    "fake_paths = glob.glob(os.path.join(face_fake_dir, \"*.*\"))\n",
    "\n",
    "all_paths = real_paths + fake_paths\n",
    "all_labels = [0] * len(real_paths) + [1] * len(fake_paths)\n",
    "\n",
    "print(f\"총 {len(all_labels)}개 이미지 경로 발견.\")\n",
    "\n",
    "# 10000개 샘플링\n",
    "print(f\"{NUM_SAMPLES}개 샘플을 샘플링...\")\n",
    "_, target_paths, _, target_labels = train_test_split(\n",
    "    all_paths, all_labels,\n",
    "    test_size= NUM_SAMPLES,\n",
    "    random_state=42,\n",
    "    stratify=all_labels\n",
    ")\n",
    "print(f\"샘플링 완료: {len(target_labels)}개 이미지 선택.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터를 7:2:1 비율로 분할합니다...\n",
      "분할 완료: Train 7000개, Validation 2000개, Test 1000개\n",
      "데이터 로더 생성 완료.\n"
     ]
    }
   ],
   "source": [
    "# 5. 7:2:1 분할 (Keras와 동일)\n",
    "print(\"데이터를 7:2:1 비율로 분할합니다...\")\n",
    "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
    "    target_paths, target_labels, test_size=0.3, random_state=42, stratify=target_labels\n",
    ")\n",
    "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
    "    temp_paths, temp_labels, test_size=(1/3), random_state=42, stratify=temp_labels\n",
    ")\n",
    "print(f\"분할 완료: Train {len(train_paths)}개, Validation {len(val_paths)}개, Test {len(test_paths)}개\")\n",
    "\n",
    "# 6. 전처리 및 데이터 로더 정의\n",
    "train_transform = transforms.Compose([\n",
    "    ResizeWithPad(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(), # 데이터 증강\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) # -1 ~ 1 정규화\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    ResizeWithPad(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_dataset = DeepfakeDataset(train_paths, train_labels, transform=train_transform)\n",
    "val_dataset = DeepfakeDataset(val_paths, val_labels, transform=val_test_transform)\n",
    "test_dataset = DeepfakeDataset(test_paths, test_labels, transform=val_test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"데이터 로더 생성 완료.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 전처리를 시작합니다 (모든 데이터를 RAM에 로드)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images to RAM: 100%|██████████| 7000/7000 [00:09<00:00, 773.24it/s]\n",
      "Loading images to RAM: 100%|██████████| 2000/2000 [00:52<00:00, 37.74it/s]\n",
      "Loading images to RAM: 100%|██████████| 1000/1000 [00:17<00:00, 57.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모든 데이터를 RAM에 로드 완료.\n",
      "데이터 로더 생성 완료.\n",
      "Loading vgg16 architecture (FROM SCRATCH)...\n",
      "=== 학습 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]:   5%|▌         | 6/110 [05:52<2:10:04, 75.04s/it, loss=0.696]"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 및 RAM에 로드 \n",
    "print(\"데이터 전처리를 시작합니다 (모든 데이터를 RAM에 로드)...\")\n",
    "\n",
    "def load_images_to_ram(paths, transform):\n",
    "    tensor_list = []\n",
    "    for path in tqdm(paths, desc=\"Loading images to RAM\"):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        tensor_list.append(transform(img))\n",
    "    return torch.stack(tensor_list)\n",
    "\n",
    "# 모든 데이터를 RAM으로 로드\n",
    "X_train = load_images_to_ram(train_paths, train_transform)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_val = load_images_to_ram(val_paths, val_test_transform)\n",
    "y_val = torch.tensor(val_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "X_test = load_images_to_ram(test_paths, val_test_transform)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(\"모든 데이터를 RAM에 로드 완료.\")\n",
    "\n",
    "# RAM 기반의 TensorDataset과 DataLoader 생성 \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# RAM에서 읽으므로 num_workers=0, pin_memory=False (이미 RAM에 있음)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"데이터 로더 생성 완료.\")\n",
    "\n",
    "# 8. 모델, 손실함수, 옵티마이저 정의\n",
    "model = get_model('vgg16', device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 9. 학습 및 평가\n",
    "model = train_model(model, train_loader, val_loader, criterion, optimizer, device, EPOCHS, PATIENCE)\n",
    "evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"총 실행 시간: {(time.time() - start_time) / 60:.2f} 분\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP9wls/CXB7IizaFou4BM03",
   "gpuType": "T4",
   "mount_file_id": "1I-ueUOeAWB7T_bRpXoU87x-OvTVOuhdg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
